# Linear Regression Project (Scikit-learn & From-Scratch)

ì´ í”„ë¡œì íŠ¸ëŠ” **ì„ í˜• íšŒê·€(Linear Regression)** ë¥¼  
â‘  **Scikit-learn ë²„ì „**ê³¼  
â‘¡ **NumPy ê¸°ë°˜ ì™„ì „ ìŠ¤í¬ë˜ì¹˜ ë²„ì „**  
ë‘ ê°€ì§€ë¡œ êµ¬í˜„í•˜ê³  ë¹„êµí•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

---

## ğŸ“‚ í´ë” êµ¬ì¡° ë° íŒŒì¼ë³„ ê¸°ëŠ¥

linreg-project/  
â”œâ”€ README.md # í”„ë¡œì íŠ¸ ê°œìš” ë° êµ¬ì¡° ì„¤ëª…  
â”œâ”€ requirements.txt # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡  
â”œâ”€ configs/ # ì„¤ì • íŒŒì¼ í´ë”  
â”‚ â”œâ”€ sklearn.yaml # scikit-learn ë²„ì „ ì„¤ì •  
â”‚ â””â”€ scratch.yaml # ìŠ¤í¬ë˜ì¹˜ ë²„ì „ ì„¤ì •  
â”‚  
â”œâ”€ common/ # ê³µìš© ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ  
â”‚ â””â”€ utils.py # ì‹œë“œ ì„¤ì •, YAML ë¡œë“œ, Pickle ì €ì¥/ë¡œë”©, ë””ë ‰í„°ë¦¬ ìƒì„± ë“±  
â”‚  
â”œâ”€ linreg_sklearn/ # scikit-learn ê¸°ë°˜ êµ¬í˜„  
â”‚ â”œâ”€ init.py  
â”‚ â”œâ”€ data.py # CSV ë˜ëŠ” sklearn ë°ì´í„°ì…‹ ë¡œë“œ  
â”‚ â”œâ”€ model.py # StandardScaler + LinearRegression íŒŒì´í”„ë¼ì¸ ì •ì˜   
â”‚ â”œâ”€ train.py # ë°ì´í„° ë¶„í• , ëª¨ë¸ í•™ìŠµ, ì €ì¥ ë° í‰ê°€ ì¶œë ¥  
â”‚ â””â”€ evaluate.py # í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ í›„ ìƒˆ ë°ì´í„° í‰ê°€  
â”‚  
â”œâ”€ linreg_scratch/ # ì™„ì „ ìŠ¤í¬ë˜ì¹˜(NumPy) êµ¬í˜„  
â”‚ â”œâ”€ init.py  
â”‚ â”œâ”€ data.py # CSV ë˜ëŠ” í•©ì„± ë°ì´í„° ìƒì„± (y = Xw + noise)  
â”‚ â”œâ”€ features.py # í‘œì¤€í™”(ìŠ¤ì¼€ì¼ë§), bias ì¶”ê°€ ê¸°ëŠ¥  
â”‚ â”œâ”€ model.py # LinearRegressionGD / LinearRegressionNE êµ¬í˜„  
â”‚ â”œâ”€ metrics.py # RMSE, MAE, RÂ² ë“± íšŒê·€ ì„±ëŠ¥ ì§€í‘œ í•¨ìˆ˜  
â”‚ â”œâ”€ train.py # í•™ìŠµ íŒŒì´í”„ë¼ì¸ (train/val/test ë¶„í• , í•™ìŠµ, ì €ì¥)  
â”‚ â””â”€ evaluate.py # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ í›„ CSV ê¸°ë°˜ í‰ê°€  
â”‚  
â””â”€ scripts/ # CLI(ëª…ë ¹ì¤„ ì‹¤í–‰) ìŠ¤í¬ë¦½íŠ¸  
â”œâ”€ train_sklearn.py # scikit-learn ë²„ì „ í•™ìŠµ ì‹¤í–‰  
â”œâ”€ eval_sklearn.py # scikit-learn ë²„ì „ í‰ê°€ ì‹¤í–‰  
â”œâ”€ train_scratch.py # ìŠ¤í¬ë˜ì¹˜ ë²„ì „ í•™ìŠµ ì‹¤í–‰  
â””â”€ eval_scratch.py # ìŠ¤í¬ë˜ì¹˜ ë²„ì „ í‰ê°€ ì‹¤í–‰  


---

## âš™ï¸ ì£¼ìš” êµ¬ì„± ì„¤ëª…

### ğŸ§© 1. configs/
- YAML í¬ë§· ì„¤ì • íŒŒì¼ë¡œ í•™ìŠµ íŒŒë¼ë¯¸í„° ë° ê²½ë¡œ ê´€ë¦¬.
- ë°ì´í„° ë¶„í•  ë¹„ìœ¨, í•™ìŠµë¥ , ì—í­ ìˆ˜, ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë“±ì„ ì§€ì •.

### ğŸ§° 2. common/utils.py
- ê³µí†µ ìœ í‹¸ í•¨ìˆ˜ ëª¨ìŒ:
  - `set_seed()`: ëœë¤ ì‹œë“œ ê³ ì •  
  - `load_yaml()`: YAML ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°  
  - `save_pickle()/load_pickle()`: ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ  
  - `ensure_dir()`: ë””ë ‰í„°ë¦¬ ìë™ ìƒì„±  

### ğŸ¤– 3. linreg_sklearn/
- Scikit-learn ê¸°ë°˜ì˜ íŒŒì´í”„ë¼ì¸ í•™ìŠµ êµ¬í˜„.
- `train.py`ëŠ” `train_test_split`ìœ¼ë¡œ ë¶„í•  í›„ `LinearRegression` í•™ìŠµ ë° ì €ì¥.
- `evaluate.py`ëŠ” ì €ì¥ëœ ëª¨ë¸ë¡œ CSV ì…ë ¥ í‰ê°€.

### ğŸ§® 4. linreg_scratch/
- NumPyë§Œìœ¼ë¡œ êµ¬í˜„ëœ ì™„ì „ ìŠ¤í¬ë˜ì¹˜ ë²„ì „.
- `model.py`  
  - `LinearRegressionGD`: ê²½ì‚¬í•˜ê°•ë²•(GD) ê¸°ë°˜ í•™ìŠµ  
  - `LinearRegressionNE`: ì •ê·œë°©ì •ì‹(Closed-form) ê¸°ë°˜ í•™ìŠµ
- `metrics.py`: RMSE, MAE, RÂ² ë“± ì§€í‘œ ê³„ì‚°.
- `train.py`: í•©ì„± ë°ì´í„° ìƒì„±, ìŠ¤ì¼€ì¼ë§, í•™ìŠµ, ì €ì¥.
- `evaluate.py`: ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì™€ CSV ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€.

### ğŸ§© 5. scripts/
- CLI í™˜ê²½ì—ì„œ ì†ì‰½ê²Œ í•™ìŠµ/í‰ê°€ ì‹¤í–‰ ê°€ëŠ¥:
  ```bash
  python scripts/train_sklearn.py --config configs/sklearn.yaml
  python scripts/eval_scratch.py --model artifacts/scratch/model.pkl --csv your.csv --target target

---

# ì„ í˜•íšŒê·€(Linear Regression) ì½”ë“œ ìŠ¤ë‹ˆí« ëª¨ìŒ
> ì‹œí—˜ ì¤‘ ë¹ ë¥´ê²Œ ì°¸ê³ í•˜ëŠ” **í•„ì‚´ ì¹˜íŠ¸ì‹œíŠ¸(README)** â€” í•µì‹¬ë§Œ ê°„ë‹¨Â·ì§§ê²Œ!  
> ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025-10-22 10:22

---

## ğŸ“Œ ë°”ë¡œ ì‚¬ìš© ìš”ì•½
- **í•„ìˆ˜ íŒ¨í‚¤ì§€**: `numpy`, `pandas`, `scikit-learn`, (ì„ íƒ) `statsmodels`
- **ë°ì´í„° ë¶„ë¦¬** â†’ **ëª¨ë¸ í•™ìŠµ** â†’ **í‰ê°€** ìˆœì„œë§Œ ê¸°ì–µ!
- **ì •ê·œí™”/í‘œì¤€í™”**: `Pipeline`ê³¼ `ColumnTransformer` ì¡°í•©ì´ ê°€ì¥ ê¹”ë”.

---

## 0) ë°ì´í„° ì˜ˆì‹œ ë§Œë“¤ê¸° (ë³µë¶™ìš©)
```python
import numpy as np, pandas as pd
np.random.seed(42)

n = 200
X = pd.DataFrame({
    "x1": np.random.randn(n),
    "x2": np.random.randn(n) * 2 + 1,
    "cat": np.random.choice(["A","B","C"], size=n)
})
y = 3.0 + 2.0*X["x1"] - 0.5*X["x2"] + (X["cat"]=="B")*1.5 + np.random.randn(n)*0.8
```

---

## 1) ê°€ì¥ ê¸°ë³¸: scikit-learn ì„ í˜•íšŒê·€
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

X_num = X[["x1","x2"]]                # ìˆ˜ì¹˜í˜•ë§Œ ìš°ì„  ì‚¬ìš©
X_tr, X_te, y_tr, y_te = train_test_split(X_num, y, test_size=0.2, random_state=42)

lr = LinearRegression().fit(X_tr, y_tr)
pred = lr.predict(X_te)

mse  = mean_squared_error(y_te, pred)
rmse = mse ** 0.5
mae  = mean_absolute_error(y_te, pred)
r2   = r2_score(y_te, pred)

print("coef_:", lr.coef_, "intercept_:", lr.intercept_)
print(f"RMSE={rmse:.3f} | MAE={mae:.3f} | R2={r2:.3f}")
```

---

## 2) ì›í•«ì¸ì½”ë”© + ìŠ¤ì¼€ì¼ë§ + íŒŒì´í”„ë¼ì¸ (ì‹¤ì „í˜•)
```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline

num_cols = ["x1","x2"]
cat_cols = ["cat"]

preprocess = ColumnTransformer([
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(drop="first"), cat_cols),
])

pipe = Pipeline([
    ("prep", preprocess),
    ("model", LinearRegression())
])

pipe.fit(X, y)
yhat = pipe.predict(X)
print("í•™ìŠµ R2:", r2_score(y, yhat))
```

---

## 3) ë‹¤í•­ íŠ¹ì„±(ë¹„ì„ í˜•ì„±) í¬í•¨
```python
from sklearn.preprocessing import PolynomialFeatures

poly_pipe = Pipeline([
    ("prep", preprocess),
    ("poly", PolynomialFeatures(degree=2, include_bias=False)),
    ("model", LinearRegression())
])
poly_pipe.fit(X, y)
print("ë‹¤í•­ R2:", r2_score(y, poly_pipe.predict(X)))
```

---

## 4) ì •ê·œí™” íšŒê·€: Ridge / Lasso / ElasticNet
```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet

ridge = Pipeline([("prep", preprocess), ("model", Ridge(alpha=1.0))]).fit(X, y)
lasso = Pipeline([("prep", preprocess), ("model", Lasso(alpha=0.05, max_iter=10000))]).fit(X, y)
enet  = Pipeline([("prep", preprocess), ("model", ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000))]).fit(X, y)

print("Ridge R2:", r2_score(y, ridge.predict(X)))
print("Lasso R2:", r2_score(y, lasso.predict(X)))
print("ElasticNet R2:", r2_score(y, enet.predict(X)))
```

**Tip**: ê·œì œê°€ ê°•í• ìˆ˜ë¡(í° `alpha`) ê³„ìˆ˜ê°€ ì¤„ê³  ê³¼ì í•© ë°©ì§€ì— ë„ì›€.  
**Lasso**ëŠ” ì¼ë¶€ ê³„ìˆ˜ë¥¼ **0ìœ¼ë¡œ** ë§Œë“¤ì–´ íŠ¹ì„± ì„ íƒ íš¨ê³¼.

---

## 5) êµì°¨ê²€ì¦ & í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
```python
from sklearn.model_selection import cross_val_score, GridSearchCV

# 5-1) êµì°¨ê²€ì¦ ì ìˆ˜
scores = cross_val_score(pipe, X, y, scoring="r2", cv=5)
print("CV R2 mean:", scores.mean(), "Â±", scores.std())

# 5-2) GridSearchë¡œ Ridge alpha ì°¾ê¸°
param_grid = {"model__alpha": [0.01, 0.1, 1.0, 10.0]}
ridge_gs = GridSearchCV(Pipeline([("prep", preprocess), ("model", Ridge())]),
                        param_grid=param_grid, scoring="r2", cv=5, n_jobs=-1)
ridge_gs.fit(X, y)
print("Best alpha:", ridge_gs.best_params_, "Best R2:", ridge_gs.best_score_)
```

---

## 6) ì§€í‘œ ëª¨ìŒ (ë³µë¶™ìš© í•¨ìˆ˜)
```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

def eval_reg(y_true, y_pred, prefix="VAL"):
    mse  = mean_squared_error(y_true, y_pred)
    rmse = mse ** 0.5
    mae  = mean_absolute_error(y_true, y_pred)
    r2   = r2_score(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100
    print(f"[{prefix}] RMSE={{rmse:.4f}} | MAE={{mae:.4f}} | R2={{r2:.4f}} | MAPE={{mape:.2f}}%")
    return dict(rmse=rmse, mae=mae, r2=r2, mape=mape)
```

---

## 7) ìˆœìˆ˜ ë„˜íŒŒì´ êµ¬í˜„(ì •ê·œë°©ì •ì‹, Normal Equation)
```python
import numpy as np

def fit_normal_eq(X, y):
    # X: (n, d), y: (n,)
    X_ = np.c_[np.ones(len(X)), X]          # bias ì¶”ê°€
    theta = np.linalg.pinv(X_.T @ X_) @ X_.T @ y
    return theta  # [intercept, w1, w2, ...]

def predict_normal_eq(X, theta):
    X_ = np.c_[np.ones(len(X)), X]
    return X_ @ theta

X_arr = X_num.values
theta = fit_normal_eq(X_arr, y)
pred  = predict_normal_eq(X_arr, theta)
print("theta:", theta)
print("R2:", r2_score(y, pred))
```

---

## 8) ê²½ì‚¬í•˜ê°•ë²•(GD) ê¸°ì´ˆ êµ¬í˜„
```python
import numpy as np

def fit_gd(X, y, lr=0.05, epochs=1000):
    X_ = np.c_[np.ones(len(X)), X]      # bias í¬í•¨
    w  = np.zeros(X_.shape[1])
    for _ in range(epochs):
        grad = (2/len(X_)) * X_.T @ (X_ @ w - y)
        w   -= lr * grad
    return w

w = fit_gd(X_num.values, y.values if hasattr(y, "values") else np.array(y), lr=0.05, epochs=2000)
print("GD w:", w)
```

---

## 9) Statsmodelsë¡œ ê³„ìˆ˜/ìœ ì˜ì„± ë³´ê³ ì„œ
```python
import statsmodels.api as sm

X_sm = sm.add_constant(X_num)  # ìƒìˆ˜í•­
model = sm.OLS(y, X_sm).fit()
print(model.summary())  # ê³„ìˆ˜, í‘œì¤€ì˜¤ì°¨, t-ê°’, p-ê°’, ì‹ ë¢°êµ¬ê°„ ë“±
```

---

## 10) ì”ì°¨ ì§„ë‹¨(ê°„ë‹¨ ì²´í¬)
```python
import numpy as np

residuals = y - lr.predict(X_num)
print("ì”ì°¨ í‰ê· â‰ˆ0?:", np.mean(residuals))
print("ì”ì°¨ ë¶„ì‚° ì¼ì •ì„±(ëŒ€ëµ):", np.var(residuals[:len(residuals)//2]), np.var(residuals[len(residuals)//2:]))
# ì •êµí•œ ê²€ì •: Breusch-Pagan, Durbin-Watson ë“±ì€ statsmodels ì°¸ê³ 
```

---

## 11) ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] **ë°ì´í„° ëˆ„ìˆ˜**: ìŠ¤ì¼€ì¼ë§/ì¸ì½”ë”©ì€ ë°˜ë“œì‹œ **trainì— fit â†’ testì— transform**.
- [ ] **ë²”ì£¼í˜• ë“œë ê¸°ì¤€**: `OneHotEncoder(drop="first")`ë¡œ ì™„ì „ ë‹¤ì¤‘ê³µì„ ì„± ë°©ì§€.
- [ ] **ìŠ¤ì¼€ì¼ë§ ëˆ„ë½**: Lasso/ElasticNetì€ **ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜**(Pipeline ê¶Œì¥).
- [ ] **í‰ê°€ ì¼ê´€ì„±**: í•­ìƒ ë™ì¼í•œ ì§€í‘œ(RMSE/R2/MAE ë“±)ë¡œ ë¹„êµ.
- [ ] **ì™¸ì  íƒ€ë‹¹ì„±**: êµì°¨ê²€ì¦ìœ¼ë¡œ ê³¼ì í•© ì—¬ë¶€ í™•ì¸.

---

## 12) ìµœì†Œ ì˜ˆì œ í…œí”Œë¦¿ (ì‹œí—˜ì¥ 10ì¤„ ì»·)
```python
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)
pipe = Pipeline([("prep", ColumnTransformer([("num", StandardScaler(), num_cols),
                                            ("cat", OneHotEncoder(drop="first"), cat_cols)])),
                 ("model", Ridge(alpha=1.0))])
pipe.fit(X_tr, y_tr)
pred = pipe.predict(X_te)
print(r2_score(y_te, pred))
```

---

### ì°¸ê³ 
- ì„ í˜•ì„± ìœ„ë°°Â·ì´ìƒì¹˜ê°€ ì‹¬í•˜ë©´: **ë¡œë²„ìŠ¤íŠ¸ íšŒê·€(HuberRegressor/RANSACRegressor)** ê²€í† 
- ëª©í‘œê°€ 0 ê·¼ì²˜ì—ì„œ ìƒëŒ€ì˜¤ì°¨ ì¤‘ìš”í•˜ë©´: **SMAPE, MAPE** ì§€í‘œ ê²€í† 
- íŠ¹ì„± ìˆ˜ê°€ ë§¤ìš° ë§ê³  í¬ì†Œ: **ElasticNet** ìš°ì„  ê³ ë ¤

